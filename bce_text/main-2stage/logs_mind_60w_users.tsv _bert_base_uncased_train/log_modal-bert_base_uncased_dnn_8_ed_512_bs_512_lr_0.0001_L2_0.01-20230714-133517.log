[INFO 2023-07-14 13:35:17,153] Namespace(batch_size=512, behaviors='mind_60w_users.tsv', bert_model_load='bert_base_uncased', cold_file='None', dataset='dataset/MIND', dnn_layers=8, drop_rate=0.1, embedding_dim=512, epoch=150, fine_tune_lr=0.0, item_tower='modal', l2_weight=0.01, label_screen='modal_bs512_ed512_lr0.0001_dp0.1_L20.01_Flr0-20230714-133517', load_ckpt_name='None', local_rank=0, logging_num=16, lr=0.0001, max_seq_len=20, min_seq_len=5, mode='train', new_file='None', news='mind_60w_items.tsv', news_attributes=['title'], num_attention_heads=2, num_words_abstract=50, num_words_body=50, num_words_title=30, num_workers=12, root_data_dir='../../', testing_num=2, transformer_block=2, word_embedding_dim=768)
[INFO 2023-07-14 13:35:17,153] load bert model...
[INFO 2023-07-14 13:36:50,793] read news...
[INFO 2023-07-14 13:37:11,219] read behaviors...
[INFO 2023-07-14 13:37:11,219] ##### news number 79707 79707 (before clearing)#####
[INFO 2023-07-14 13:37:11,219] ##### min seq len 5, max seq len 20#####
[INFO 2023-07-14 13:37:11,220] rebuild user seqs...
[INFO 2023-07-14 13:37:14,971] ##### pairs_num 9667540
[INFO 2023-07-14 13:37:15,005] ##### items after clearing 79707, 79707, 79707 #####
[INFO 2023-07-14 13:37:25,810] ##### user seqs after clearing 630235, 630235, 630235#####
[INFO 2023-07-14 13:37:25,810] ##### train pairs 8407070 #####
[INFO 2023-07-14 13:37:25,810] ##### validation pairs 630235 #####
[INFO 2023-07-14 13:37:25,810] ##### test pairs 630235 #####
[INFO 2023-07-14 13:37:29,746] combine news information...
[INFO 2023-07-14 13:37:30,040] Bert Encoder...
[INFO 2023-07-14 13:37:30,195] get bert output...
[INFO 2023-07-14 13:37:51,763] build dataset...
[INFO 2023-07-14 13:37:51,764] build DDP sampler...
[INFO 2023-07-14 13:37:51,764] build dataloader...
[INFO 2023-07-14 13:37:51,764] build model...
[INFO 2023-07-14 13:37:56,398] MLP_Layers(
  (mlp_layers): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=512, out_features=512, bias=True)
    (2): GELU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=512, out_features=512, bias=True)
    (5): GELU()
    (6): Dropout(p=0.1, inplace=False)
    (7): Linear(in_features=512, out_features=512, bias=True)
    (8): GELU()
    (9): Dropout(p=0.1, inplace=False)
    (10): Linear(in_features=512, out_features=512, bias=True)
    (11): GELU()
    (12): Dropout(p=0.1, inplace=False)
    (13): Linear(in_features=512, out_features=512, bias=True)
    (14): GELU()
    (15): Dropout(p=0.1, inplace=False)
    (16): Linear(in_features=512, out_features=512, bias=True)
    (17): GELU()
    (18): Dropout(p=0.1, inplace=False)
    (19): Linear(in_features=512, out_features=512, bias=True)
    (20): GELU()
    (21): Dropout(p=0.1, inplace=False)
    (22): Linear(in_features=512, out_features=512, bias=True)
    (23): GELU()
  )
)
[INFO 2023-07-14 13:37:56,401] ##### total_num 325175808 #####
[INFO 2023-07-14 13:37:56,401] ##### trainable_num 325175808 #####
[INFO 2023-07-14 13:37:56,401] 

[INFO 2023-07-14 13:37:56,401] Training...
[INFO 2023-07-14 13:37:56,401] ##### total_num 325175808 #####
[INFO 2023-07-14 13:37:56,402] ##### trainable_num 325175808 #####
[INFO 2023-07-14 13:37:56,402] ##### all 16421 steps #####
[INFO 2023-07-14 13:37:56,402] ##### 16 logs/epoch; 1026 steps/log #####
[INFO 2023-07-14 13:37:56,402] ##### 2 tests/epoch; 8210 steps/test #####
[INFO 2023-07-14 13:37:56,402] 

[INFO 2023-07-14 13:37:56,402] epoch 1 start
[INFO 2023-07-14 13:37:56,402] 
[INFO 2023-07-14 13:38:56,602] cnt: 1026, Ed: 525312, batch loss: 0.68489, sum loss: 702.69348
[INFO 2023-07-14 13:39:55,561] cnt: 2052, Ed: 1050624, batch loss: 0.65659, sum loss: 1347.31873
[INFO 2023-07-14 13:40:54,107] cnt: 3078, Ed: 1575936, batch loss: 0.62351, sum loss: 1919.14966
[INFO 2023-07-14 13:41:53,068] cnt: 4104, Ed: 2101248, batch loss: 0.59264, sum loss: 2432.19287
[INFO 2023-07-14 13:42:51,784] cnt: 5130, Ed: 2626560, batch loss: 0.56616, sum loss: 2904.41846
[INFO 2023-07-14 13:43:50,320] cnt: 6156, Ed: 3151872, batch loss: 0.54359, sum loss: 3346.35425
[INFO 2023-07-14 13:44:49,347] cnt: 7182, Ed: 3677184, batch loss: 0.52420, sum loss: 3764.81982
[INFO 2023-07-14 13:45:47,983] cnt: 8208, Ed: 4202496, batch loss: 0.50768, sum loss: 4166.99756
[INFO 2023-07-14 13:45:48,097] 
[INFO 2023-07-14 13:45:48,097] Validating...
[INFO 2023-07-14 13:45:56,840] train_methods   Hit10	nDCG10
