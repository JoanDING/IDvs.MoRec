[INFO 2023-07-14 13:49:35,359] Namespace(batch_size=512, behaviors='mind_60w_users.tsv', bert_model_load='bert_base_uncased', cold_file='None', dataset='dataset/MIND', dnn_layers=8, drop_rate=0.1, embedding_dim=512, epoch=150, fine_tune_lr=0.0, item_tower='modal', l2_weight=0.01, label_screen='modal_bs512_ed512_lr0.0001_dp0.1_L20.01_Flr0-20230714-134935', load_ckpt_name='None', local_rank=0, logging_num=16, lr=0.0001, max_seq_len=20, min_seq_len=5, mode='train', new_file='None', news='mind_60w_items.tsv', news_attributes=['title'], num_attention_heads=2, num_words_abstract=50, num_words_body=50, num_words_title=30, num_workers=12, root_data_dir='../../', testing_num=2, transformer_block=2, word_embedding_dim=768)
[INFO 2023-07-14 13:49:35,359] load bert model...
[INFO 2023-07-14 13:49:44,724] read news...
[INFO 2023-07-14 13:50:04,932] read behaviors...
[INFO 2023-07-14 13:50:04,932] ##### news number 79707 79707 (before clearing)#####
[INFO 2023-07-14 13:50:04,932] ##### min seq len 5, max seq len 20#####
[INFO 2023-07-14 13:50:04,932] rebuild user seqs...
[INFO 2023-07-14 13:50:08,686] ##### pairs_num 9667540
[INFO 2023-07-14 13:50:08,721] ##### items after clearing 79707, 79707, 79707 #####
[INFO 2023-07-14 13:50:19,582] ##### user seqs after clearing 630235, 630235, 630235#####
[INFO 2023-07-14 13:50:19,583] ##### train pairs 8407070 #####
[INFO 2023-07-14 13:50:19,583] ##### validation pairs 630235 #####
[INFO 2023-07-14 13:50:19,583] ##### test pairs 630235 #####
[INFO 2023-07-14 13:50:23,487] combine news information...
[INFO 2023-07-14 13:50:23,767] Bert Encoder...
[INFO 2023-07-14 13:50:23,921] get bert output...
[INFO 2023-07-14 13:50:45,486] build dataset...
[INFO 2023-07-14 13:50:45,487] build DDP sampler...
[INFO 2023-07-14 13:50:45,487] build dataloader...
[INFO 2023-07-14 13:50:45,487] build model...
[INFO 2023-07-14 13:50:50,088] MLP_Layers(
  (mlp_layers): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=512, out_features=512, bias=True)
    (2): GELU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=512, out_features=512, bias=True)
    (5): GELU()
    (6): Dropout(p=0.1, inplace=False)
    (7): Linear(in_features=512, out_features=512, bias=True)
    (8): GELU()
    (9): Dropout(p=0.1, inplace=False)
    (10): Linear(in_features=512, out_features=512, bias=True)
    (11): GELU()
    (12): Dropout(p=0.1, inplace=False)
    (13): Linear(in_features=512, out_features=512, bias=True)
    (14): GELU()
    (15): Dropout(p=0.1, inplace=False)
    (16): Linear(in_features=512, out_features=512, bias=True)
    (17): GELU()
    (18): Dropout(p=0.1, inplace=False)
    (19): Linear(in_features=512, out_features=512, bias=True)
    (20): GELU()
    (21): Dropout(p=0.1, inplace=False)
    (22): Linear(in_features=512, out_features=512, bias=True)
    (23): GELU()
  )
)
[INFO 2023-07-14 13:50:50,091] ##### total_num 325175808 #####
[INFO 2023-07-14 13:50:50,094] ##### trainable_num 325175808 #####
[INFO 2023-07-14 13:50:50,094] 

[INFO 2023-07-14 13:50:50,094] Training...
[INFO 2023-07-14 13:50:50,094] ##### total_num 325175808 #####
[INFO 2023-07-14 13:50:50,094] ##### trainable_num 325175808 #####
[INFO 2023-07-14 13:50:50,094] ##### all 16421 steps #####
[INFO 2023-07-14 13:50:50,094] ##### 16 logs/epoch; 1026 steps/log #####
[INFO 2023-07-14 13:50:50,094] ##### 2 tests/epoch; 8210 steps/test #####
[INFO 2023-07-14 13:50:50,094] 

[INFO 2023-07-14 13:50:50,094] epoch 1 start
[INFO 2023-07-14 13:50:50,094] 
[INFO 2023-07-14 13:51:48,370] cnt: 1026, Ed: 525312, batch loss: 0.68489, sum loss: 702.69348
[INFO 2023-07-14 13:52:45,188] cnt: 2052, Ed: 1050624, batch loss: 0.65659, sum loss: 1347.31873
[INFO 2023-07-14 13:53:43,711] cnt: 3078, Ed: 1575936, batch loss: 0.62351, sum loss: 1919.14966
[INFO 2023-07-14 13:54:42,530] cnt: 4104, Ed: 2101248, batch loss: 0.59264, sum loss: 2432.19287
[INFO 2023-07-14 13:55:41,274] cnt: 5130, Ed: 2626560, batch loss: 0.56616, sum loss: 2904.41846
[INFO 2023-07-14 13:56:39,894] cnt: 6156, Ed: 3151872, batch loss: 0.54359, sum loss: 3346.35425
[INFO 2023-07-14 13:57:39,099] cnt: 7182, Ed: 3677184, batch loss: 0.52420, sum loss: 3764.81982
[INFO 2023-07-14 13:58:37,783] cnt: 8208, Ed: 4202496, batch loss: 0.50768, sum loss: 4166.99756
[INFO 2023-07-14 13:58:37,897] 
[INFO 2023-07-14 14:02:26,239] Validating...
[INFO 2023-07-14 14:02:35,737] train_methods   Hit10	nDCG10
